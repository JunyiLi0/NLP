{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3575df53",
   "metadata": {},
   "source": [
    "# Lab 02\n",
    "## Introduction\n",
    "This project's goal is to code a sentiment classifier on the IMDB sentiment dataset. The IMDB sentiment [dataset](https://huggingface.co/datasets/imdb) is a collection of 50K movie reviews, annotated as positive or negative, and split in two sets of equal size: a training and a test set. Both set have an equal number of positive and negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ccd6f",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a164ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/junyi/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f460a85702544e180d2aa3cf5f44156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6f4f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 25000, 'test': 25000, 'unsupervised': 50000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9c140",
   "metadata": {},
   "source": [
    "### 1. How many splits does the dataset has ?\n",
    "#### The dataset has 3 splits : train, test and unsupervised.\n",
    "\n",
    "### 2. How big are these splits ?\n",
    "#### They represent respectively 25000, 25000 and 50000 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7f63e",
   "metadata": {},
   "source": [
    "### 3. What is the proportion of each class on the supervised splits ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d4a8c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\junyi\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-50ca35f45cb9d15a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\junyi\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-3cd99f281cbb7e3f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the train and test splits, there are respectively 12500/25000 and 12500/25000 positive ratings\n"
     ]
    }
   ],
   "source": [
    "positiveTrain = dataset['train'].filter(lambda example: example['label'] == 1).num_rows\n",
    "positiveTest = dataset['test'].filter(lambda example: example['label'] == 1).num_rows\n",
    "print(\"In the train and test splits, there are respectively \"+ str(positiveTrain) + \"/25000 and \" + str(positiveTest) + \"/25000 positive ratings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fa35e",
   "metadata": {},
   "source": [
    "#### Both supervised  have an equal number of positive and negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c616bc",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "### 1. Processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21edaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what s your name  i m ba yes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def process(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts all uppercase letters to lowercase, replaces all punctuation marks with spaces, and returns the processed string.\n",
    "    \"\"\"\n",
    "    \n",
    "    lowercase_txt = txt.lower()\n",
    "    \n",
    "    # create a translation table using maketrans method\n",
    "    replace_punctuation = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    # use the translate method to replace the punctuation\n",
    "    processed_txt = lowercase_txt.translate(replace_punctuation)\n",
    "    return processed_txt\n",
    "process(\"What's your name? I'm Ba-yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d36eff",
   "metadata": {},
   "source": [
    "### 2. Our Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f98a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "\n",
    "def train_naive_bayes(documents: list[str], classes: list[int], process) -> tuple[set[int, float], set[tuple[str, int], float], list[str]]:\n",
    "    \"\"\"\n",
    "    Trains a Naive Bayes classifier on a list of labeled documents.\n",
    "\n",
    "    Args:\n",
    "    - documents (list): A list of dictionaries containing the text and label for each document.\n",
    "    - classes (list): A list of the possible class labels (0 or/and 1).\n",
    "\n",
    "    Returns:\n",
    "    - log_prior (dict): A dictionary containing the log prior probabilities for each class.\n",
    "    - log_likelihood (dict): A defaultdict containing the log likelihood probabilities\n",
    "    for each word in the vocabulary given each class.\n",
    "    - vocabulary (set): A set of words in the vocabulary.\n",
    "    - class_word_sets (dict): a dictionary containing sets of words in each class.\n",
    "    \"\"\"\n",
    "    n_doc = len(documents)\n",
    "    log_prior = {}\n",
    "    whole_vocabulary = set([word for d in documents for word in process(d['text']).split()])\n",
    "    vocabulary = sorted({s for s in whole_vocabulary if s.isalpha()})\n",
    "    log_likelihood = defaultdict(lambda: math.log(1/len(vocabulary)))\n",
    "    bigdoc = {}\n",
    "    word_counts = {}\n",
    "    class_word_sets = {}\n",
    "    # Calculate P(c) terms\n",
    "    for c in classes:\n",
    "        n_c = len([d for d in documents if d['label'] == c])\n",
    "        log_prior[c] = math.log(n_c / n_doc)\n",
    "\n",
    "    # Calculate P(w|c) terms\n",
    "        bigdoc = [process(d['text']).split() for d in documents if d['label'] == c]\n",
    "        word_counts[c] = Counter([word for doc in bigdoc for word in doc])\n",
    "        class_word_sets[c] = set(word_counts[c].keys())\n",
    "        total_count = sum(word_counts[c].values())\n",
    "        for word in vocabulary:\n",
    "            count_w_c = word_counts[c][word]\n",
    "            log_likelihood[(word, c)] = math.log((count_w_c + 1) / (total_count + len(vocabulary)))\n",
    "\n",
    "    return log_prior, log_likelihood, vocabulary, class_word_sets\n",
    "\n",
    "\n",
    "def test_naive_bayes(test_txt, log_prior, log_likelihood, classes, vocabulary, class_word_sets):\n",
    "    \"\"\"\n",
    "    Predicts the label of a given test document using a trained Naive Bayes classifier.\n",
    "    \"\"\"\n",
    "    # Calculate sum[c] for each class c\n",
    "    sum_c = {}\n",
    "    test_words = set(test_txt.split())\n",
    "    for c in classes:\n",
    "        if test_words & class_word_sets[c]:\n",
    "            sum_c[c] = log_prior[c]\n",
    "            for word in test_words & class_word_sets[c]:\n",
    "                sum_c[c] += log_likelihood[(word, c)]\n",
    "    \n",
    "    # Return the class with highest sum[c]\n",
    "    return max(sum_c, key=sum_c.get)\n",
    "\n",
    "def test_accuracy(test_set, log_prior, log_likelihood, classes, vocabulary, class_word_sets, process):\n",
    "    \"\"\"\n",
    "    Tests the accuracy of a trained Naive Bayes classifier on a given test set.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: The accuracy of the classifier on the test set as a fraction between 0 and 1.\n",
    "    \"\"\"\n",
    "    true = 0\n",
    "    total = 0\n",
    "    for test_doc in test_set:\n",
    "        test_class = test_naive_bayes(process(test_doc[\"text\"]), log_prior, log_likelihood, classes, vocabulary, class_word_sets)\n",
    "        if test_doc[\"label\"] == test_class:\n",
    "            true += 1\n",
    "        total += 1\n",
    "    return true/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484b66f",
   "metadata": {},
   "source": [
    "### 3. With Sickit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e160b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sickit_class = Pipeline([\n",
    "    ('vect', CountVectorizer()),  # Tokenize and vectorize the data\n",
    "    ('clf', MultinomialNB()),  # Train the naive bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e9b2e",
   "metadata": {},
   "source": [
    "### 4. Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f7708",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d08e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prior, log_likelihood, vocabulary, class_word_sets = train_naive_bayes(dataset['train'], [0, 1], process)\n",
    "sickit_class.fit(dataset['train']['text'], dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9db44d",
   "metadata": {},
   "source": [
    "#### Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fbdbad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on both training and test set, for the scikit-learn implementation are respectively 0.89812 and 0.81356\n",
      "The accuracy on both training and test set, for ours are respectively 0.35104 and 0.67752\n"
     ]
    }
   ],
   "source": [
    "sickit_test = sickit_class.score(dataset['test']['text'], dataset['test']['label'])\n",
    "sickit_train = sickit_class.score(dataset['train']['text'], dataset['test']['label'])\n",
    "print(\"The accuracy on both training and test set, for the scikit-learn implementation are respectively \" + str(sickit_train) +\n",
    "      ' and ' + str(sickit_test))\n",
    "ours_train = test_accuracy(dataset['train'], log_prior, log_likelihood, [0, 1], vocabulary, class_word_sets, process)\n",
    "ours_test = test_accuracy(dataset['test'], log_prior, log_likelihood, [0, 1], vocabulary, class_word_sets, process)\n",
    "print(\"The accuracy on both training and test set, for ours are respectively \" + str(ours_train) + ' and ' + str(ours_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64d845",
   "metadata": {},
   "source": [
    "### 5. Most likely, the scikit-learn implementation will give better results. Looking at the documentation, explain why it could be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727186a",
   "metadata": {},
   "source": [
    "By using CountVectorizer, scikit will process the text better than our process(txt) function.\n",
    "\n",
    "Then, MultinomialNB has 2 extra default parameters :\n",
    "\n",
    "* alpha : Additive (Laplace/Lidstone) smoothing parameter.\n",
    "* fit_prior : Learn class prior probabilities.\n",
    "\n",
    "which will increase the accuracy of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c82ea0",
   "metadata": {},
   "source": [
    "### 6. Why is accuracy a sufficient measure of evaluation here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50456f77",
   "metadata": {},
   "source": [
    "A balanced dataset ensures that the classifier does not have bias towards one class over the other, making accuracy an appropriate metric for evaluating the model's overall performance.\n",
    "In imbalanced datasets or when different misclassification costs are involved, other measures like precision, recall, F1-score are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573f504",
   "metadata": {},
   "source": [
    "### 7. Using one of the implementation, take at least 2 wrongly classified example from the test set and try explaining why the model failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff612b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blind Date (Columbia Pictures, 1934), was a decent film, but I have a few issues with this film. First of all, I don't fault the actors in this film at all, but more or less, I have a problem with the script. Also, I understand that this film was made in the 1930's and people were looking to escape reality, but the script made Ann Sothern's character look weak. She kept going back and forth between suitors and I felt as though she should have stayed with Paul Kelly's character in the end. He truly did care about her and her family and would have done anything for her and he did by giving her up in the end to fickle Neil Hamilton who in my opinion was only out for a good time. Paul Kelly's character, although a workaholic was a man of integrity and truly loved Kitty (Ann Sothern) as opposed to Neil Hamilton, while he did like her a lot, I didn't see the depth of love that he had for her character. The production values were great, but the script could have used a little work.\n",
      "0 1\n",
      "Ben, (Rupert Grint), is a deeply unhappy adolescent, the son of his unhappily married parents. His father, (Nicholas Farrell), is a vicar and his mother, (Laura Linney), is ... well, let's just say she's a somewhat hypocritical soldier in Jesus' army. It's only when he takes a summer job as an assistant to a foul-mouthed, eccentric, once-famous and now-forgotten actress Evie Walton, (Julie Walters), that he finally finds himself in true 'Harold and Maude' fashion. Of course, Evie is deeply unhappy herself and it's only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness.<br /><br />Of course it's corny and sentimental and very predictable but it has a hard side to it, too and Walters, who could sleep-walk her way through this sort of thing if she wanted, is excellent. It's when she puts the craziness to one side and finds the pathos in the character, (like hitting the bottle and throwing up in the sink), that she's at her best. The problem is she's the only interesting character in the film (and it's not because of the script which doesn't do anybody any favours). Grint, on the other hand, isn't just unhappy; he's a bit of a bore as well while Linney's starched bitch is completely one-dimensional. (Still, she's got the English accent off pat). The best that can be said for it is that it's mildly enjoyable - with the emphasis on the mildly.\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = sickit_class.predict(dataset['test']['text'])\n",
    "count = 0\n",
    "examples = ['','']\n",
    "labels = [-1, -1]\n",
    "true_labels = [-1, -1]\n",
    "for i in range(len(dataset['test']['label'])):\n",
    "    if count == 2:\n",
    "        break\n",
    "    if predicted_labels[i] != dataset['test']['label'][i]:\n",
    "        examples[count] = dataset['test']['text'][i]\n",
    "        labels[count] = predicted_labels[i]\n",
    "        true_labels[count] = dataset['test']['label'][i]\n",
    "        count += 1;\n",
    "print(examples[0])\n",
    "print(true_labels[0], labels[0])\n",
    "print(examples[1])\n",
    "print(true_labels[1], labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0724d6",
   "metadata": {},
   "source": [
    "#### The model failed because in their reviews, they don't totaly take position.\n",
    "\n",
    "#### (The production values were great, but)\n",
    "\n",
    "#### (it's mildly enjoyable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543005ea",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b017f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "438b2291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\junyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to download a package for word tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d69b6",
   "metadata": {},
   "source": [
    "### 1. Add stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7a33dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at first histor linguist serv as the cornerston of compar linguist primarili as a tool for linguist reconstruct 5 scholar were concern chiefli with establish languag famili and reconstruct prehistor proto languag use the compar method and intern reconstruct'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(text: str) -> str:\n",
    "    '''\n",
    "    Converts all uppercase letters to lowercase, replaces all punctuation marks with spaces and stems the text.\n",
    "    Return the processed string.\n",
    "    '''\n",
    "    text = process(text)\n",
    "    \" \".join(word_tokenize(text))\n",
    "    re_word = re.compile(r\"^\\w+$\")\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for word in word_tokenize(text.lower()) if re_word.match(word)]\n",
    "    return \" \".join(stemmed)\n",
    "\n",
    "text = \"At first, historical linguistics served as the cornerstone of comparative linguistics primarily as a tool for linguistic reconstruction.[5] Scholars were concerned chiefly with establishing language families and reconstructing prehistoric proto-languages, using the comparative method and internal reconstruction.\"\n",
    "stemming(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327cdf07",
   "metadata": {},
   "source": [
    "### 2. Train and evaluate your model again with these pretreatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84494eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set, for our implementation with the stemming is 0.70272\n"
     ]
    }
   ],
   "source": [
    "log_prior, log_likelihood, vocabulary, class_word_sets = train_naive_bayes(dataset['train'], [0, 1], stemming)\n",
    "ours_test = test_accuracy(dataset['test'], log_prior, log_likelihood, [0, 1], vocabulary, class_word_sets, stemming)\n",
    "print(\"The accuracy on the test set, for our implementation with the stemming is \" + str(ours_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a44fa",
   "metadata": {},
   "source": [
    "### 3. Are the results better or worse? Try explaining why the accuracy changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb054920",
   "metadata": {},
   "source": [
    "#### The results are better since we have less vocabulary as the words with the same meaning are merged. \n",
    "#### Thus, we have more training data for a word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
